---
title: LuxDiT
author: liliazhen669
date: 2025-09-07 20:00:00 +0800
categories: [Learning, Relighting]
tags: [paper] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

# Lighting Estimation with Video Diffusion Transformer

![fig-1](assets/img/luxdit/fig1.png)

## Method

### Data Strategy

本文监督训练的模型需要数据对: $(\mathrm{I}, \mathrm{E}_{ldr}, \mathrm{E}_{log})$, 其中 $\mathrm{I}$ 是输入的 LDR 图像，而 $\mathrm{E}_{ldr}, \mathrm{E}_{log}$  是输入的 HDR 经过色调映射后的图像。为了应对真实世界中的 HDR 图像的稀缺性，本文利用了三种补充数据：合成渲染数据，HDR 全景图像，以及 LDR 全景视频。

**Synthetic rendering data.** 为了使用物理上精确的光照线索来监督光照预测，本文通过渲染由 HDR 环境贴图照亮的随机 3D 场景来生成合成数据。每个场景包含：(i) 一个具有随机分配的 PBR 材质的地平面（ground plane）；(ii) 从 Objaverse Dataset 中采样的 3D 对象；以及 (iii) 具有不同材质的简单几何基元（geometric primitives），例如球体、立方体和圆柱体。本文使用随机的摄像机轨迹和环境贴图旋转为每个场景渲染多帧。尽管这些场景很简单，但它们展现出多样的光照效果（lighting effect），包括投射阴影、镜面高光和相互反射，所有这些都与真实的 HDR 光照相结合。在消融实验中，作者发现这些数据对于模型学习准确的阴影线索和光源位置至关重要。

**HDR panorama images.** 本文从 HDR 环境图中使用数据增强采样全景裁剪图来得到训练数据对。具体地，给定一张全景图，首先随机采样包括极角，仰角，视场角，曝光尺度这些相机参数。这些参数共同定义了一个虚拟相机，随机用这个相机将 HDR 环境图投影以得到 LDR 透视视图 $\mathrm{I}$。相应的 HDR 环境图作为 ground truth 光照目标 $\mathrm{E}$ 。 为了支持视频中的时间这一个维度，本文还通过将每个时间的相机姿态进行平滑来生成多帧序列。

**LDR panorama videos.** 为了确保动态全景环境图的生成，训练数据中还包含了 LDR 全景视频。对于这类数据，一般情况下 ground truth HDR 环境图是不能够得到的，这类视频数据的数据对构建方式为：$(\mathrm{I}, \mathrm{E}_{ldr}, \emptyset)$ 。类似地，全景视频使用随机的相机参数被投影为透视视图视频（perspective view video）。尽管这类数据对缺失了 HDR 强度这一信息，但通过自然图像统计数据、运动模式和多​​样化的真实世界光照条件下，这些数据提高了模型的鲁棒性和时间一致性。具体地，本文使用来自 WEB360 数据集的 2,000 个全景视频进行训练，并保留 114 个视频进行评估。





