---
title: LCVD
author: liliazhen669
date: 2025-05-24 12:00:00 +0800
categories: [Learning, Relighting]
tags: [paper] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

# LCVD:High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model

## Abstract

现存的portrait animation 方法无法实现重光照，这是因为这些方法没有分离出本质特征（人物identit和appearance）和外在特征（pose和lighting）。本文提出的LCVD（Lighting Controllable Video Diffusion model）通过在预训练的图像到视频扩散模型的特征空间中设置各自的子空间来区分这些特征类型，从而解决了这一限制。具体而言，使用人像的 3D mesh、pose和lighting rendered shading hints来表示外在属性，而reference则表示内在属性。在训练阶段，本文使用reference adapter将reference映射到内在特征子空间，并使用shading adapter将shading hints映射到外在特征子空间。通过合并来自这些子空间的特征，该模型实现了对动画中光照、姿势和表情的精细控制。除此之外，广泛的实验评估表明，LCVD 在照明真实感、图像质量和视频一致性方面优于最先进的方法，为可重新照明的portrait animation 树立了新的benchmark。


## Introduction

基于 I2V（Image TO Video） 扩散模型，本文提出了一种新颖的可控光照视频扩散模型 (LCVD)，以实现高保真、可重调光照的portrait animation。**首先**，使用现成的模型（Learning an animatable detailed 3d face model from in-thewild images）提取目标肖像的 3D mesh、pose和球谐函数光照系数（spherical harmonics lighting coefficients），并将其渲染为包含光照和姿态信息的shading hints。**然后**在训练阶段，为了实现姿态对齐和光照控制，使用shading adapter将这些shading hints映射到外部特征子空间，通过在shading hints和目标肖像之间建立映射来表示外部肖像属性。为了保留内在的identity和appearance，使用reference adapter将reference image映射到内部特征子空间，通过在初始帧和后续帧之间建立映射来表示内部肖像属性。**最后**在推理阶段，将两个子空间的特征合并，并将它们输入到 I2V 扩散模型中，以生成具有指定光照、姿势、身份和外观的肖像。为了进一步控制光照强度，采用多条件无分类器引导，以强调shading adapter的影响，并降低参考对重新光照的影响。

**主要贡献**如下：
- 引入了可控光照视频扩散模型 (LCVD)，这是一个基于漫射的可重光照人像动画框架，它克服了当前人像动画方法在制作人像动画时无法操控光照的局限性。
- 提出了一个shading adapter和一个reference adapter，用于构建外部和内部面部特征的特征子空间。通过合并这两个子空间，可以引导 I2V 模型实现可重光照的人像动画。
- 大量实验表明，LCVD 模型超越了最先进的方法，在光照效果、图像质量和视频一致性相关指标方面均有显著提升。

## Method

![fig-2](assets/img/lcvd/fig2.png)

本文提出的可控重光照portrait animation流程包含两个阶段。**首先**，在训练阶段，利用两个适配器，在预训练的 I2V 模型特征空间内构建人像固有特征子空间和外在特征子空间。**然后**，在重新光照和动画阶段，修改外在特征子空间并将其与固有特征子空间合并，以实现可重新光照的人像动画，如图所示。

- *Portrait Attributes Subspace Modeling Stage*：采用现成的*DECA*模型对输入视频的每一帧进行编码，提取光照、姿势和形状等关键参数，然后将其渲染为shading hints。shading hints和reference分别经过对应的adapter处理后，会被随机选择，每次训练迭代可能包含其中一个、两个或两个都不包含，用于重光照。合成的特征随后被输入到稳定视频扩散模型进行自监督训练。此阶段的目标是通过联合优化两个adapter，对外部和内部特征子空间进行建模。
- *Relighting and Animation Stage*：使用视频中的肖像姿态、参考图像中的形状以及目标光照的球面谐波系数来渲染shading hints。然后，将阴影适配器和参考适配器的输出组合起来，形成条件集，并采用多条件无分类器引导，通过修改引导强度来调整外部特征引导方向的大小，从而生成光照可控的肖像动画结果。

### Portrait Attributes Subspace Modeling

当前的portrait animation方法可以由用户提供的姿势信息驱动。然而，由于肖像的内在特征和外在特征在自监督训练过程中相互**耦合**，因此操纵光照需要修改外在特征。这种耦合使得在肖像动画过程中独立调整光照变得困难。因此，分离肖像的内在特征和外在特征对于在肖像动画中实现有效的光照控制是一项重大挑战。为了解决这个问题，本文设计了一个shading adapter和一个reference adapter，用于在训练阶段在Stable Video Diffusion（SVD）特征空间内构建外在特征子空间和内在特征子空间。**首先**，使用参数模型**FLAME**作为先验，对人物肖像的形状和姿态属性进行建模。具体地，使用 DECA 来估计这些参数，同时 DECA 还能够预测二阶球谐函数的光照系数 $l \in \mathbb{R} ^{|l|}$。

**然后**，使用球谐函数渲染 3D 人脸网格，生成一个光照阴影人脸，称为shading hints。将输入视频片段处理成一系列阴影提示，这些shading hints与reference一起，由两个adapter独立转换为特征 $\mathbf{F}_{s}$ 和 $\mathbf{F}_{r}$。为了建立这两个特征子空间，$\mathbf{F}_{s}$ 和 $\mathbf{F}_{r}$ 与随机系数 $\left \{ \alpha, \beta|\alpha, \beta \in \left \{ 0,1 \right \}  \right \}$ 重新组合，并输入到SVD中。这种方法有效地使 SVD 能够探索其特征空间内的外部特征子空间和内部特征子空间。

### Lighting-Guided Video Diffusion Model

选择稳定视频扩散模型 (SVD) 作为 LCVD 方法的先验模型。然而，SVD 是一个图像引导的视频生成模型，其输入为图像 $I \in \mathbb{R}^{H\times W\times 3}$
。该图像首先由 CLIP 的视觉编码器编码，并传入 SVD 的交叉注意模块。同时，图像由变分自编码器 (VAE) 编码为潜在表示 $z_0 = \mathcal{E}(I) \in \mathbb{R}^{h \times w \times c}$。然后，该潜在表示 $z_0$ 被复制 T 次，并沿通道维度与噪声 $\hat{z} \in \mathbb{R}^{T \times h \times w \times c}连接，得到 $z_{t} \in \mathbb{R}^{T \times h \times w \times c}$。然后将得到的 $z_t$ 输入到 3D UNet 中，该网络对输入进行逐步去噪，生成 T 帧的视频。这里，设定 $h = H/8,w = W/8$ 和 $c = 4$。例如，当一张狗在街上行走的图像输入 SVD 时，模型会根据原始图像预测接下来的 T 帧狗的图像。因此，SVD 生成的后续帧会继承原始图像中的物体和光照条件。为了消除原始图像光照对重新光照结果的影响，如图 2 所示，使用遮罩将肖像从参考图像的潜在空间中移除。在训练阶段，使用遮罩 M 移除肖像，以补偿使用参考适配器丢失的身份和外观信息。此外，将每帧的肖像遮罩合并到损失函数中，使模型更加关注肖像区域。损失函数定义如下：

$$\mathcal{L}_p=\mathbb{E}\left[\left\|\left(1-\mathcal{M}\right)\left(\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t,t,\mathbf{c}\right)\right)\right\|\right],$$

其中 $\epsilon \sim \mathcal{N}(0,I) \in \mathbb{R}^{T \times h \times w \times c}$ ，以及portrait mask $\mathcal{M} \in \left \{ 0,1 \right \}^{T \times h \times w \times c}$ ，最终的损失函数如下：

$$\mathcal{L}=\mathcal{L}_{p}+\mathcal{L}_{LDM}.$$

### Lighting Controllable Portrait Animation

在重新光照和动画阶段，整合了参考图像、视频片段和目标光照。当参考图像中的肖像与视频片段中的肖像对应同一个人时，利用 DECA 从视频肖像中提取姿势信息，并从参考图像中提取形状信息，然后根据从目标光照导出的光照系数渲染一系列阴影提示。然而，如果视频和参考图像中的肖像不代表同一个人，会引入一个运动对齐模块来降低视频肖像身份泄露的风险，因为这可能会影响生成的输出质量（有关运动对齐的更多详细信息，请参阅补充材料）。之后，将阴影提示和参考图像输入到着色适配器和参考适配器中。

鉴于参考图中的肖像本身就包含自身的光照信息，直接组合特征可能会导致原始光照主导阴影提示，从而导致无效的重新光照。为了解决这个问题，采用了 Composer的概念。这使得能够通过在一定条件下调整光照引导的方向来实现肖像光照的操控。公式如下：

$$\hat{\boldsymbol{\epsilon}}_\theta(\mathbf{z}_t,\mathbf{c})=\omega\left(\boldsymbol{\epsilon}_\theta(\mathbf{z}_t,\mathbf{c}_2)-\boldsymbol{\epsilon}_\theta(\mathbf{z}_t,\mathbf{c}_1)\right)+\boldsymbol{\epsilon}_\theta(\mathbf{z}_t,\mathbf{c}_1),$$

这里，$c_1$ 和 $c_2$ 是两组条件。如果某个条件存在于 $c_2$ 中但不存在于 $c_1$ 中，则其强度会通过权重 $\omega$ 增强。$\omega$ 越大，条件越强。如果某个条件同时存在于 $c_1$ 和 $c_2$ 中，$\omega$ 则不起作用，条件强度默认为 1.0。这样，可以设 $c_{2}= \mathbf{F}_{s} + \mathbf{F}_{r}$ 且 $c_{1} = \mathbf{F}_{r}$，其中 $\mathbf{F}_{s}$ 和 $\mathbf{F}_{r}$ 分别是来自着色适配器和参考适配器的特征。由于 Fs 存在于 c2 中但不存在于 $c_{1}$ 中，可以通过调整 $\omega$ 来增强外部特征的强度。同时，由于 $c_1$ 和 $c_2$ 都包含 $\mathbf{F}_{r}$，因此参考图像的肖像特征保持不变。这使能够利用无需分类器的条件组合指导来实现可重现的肖像动画。