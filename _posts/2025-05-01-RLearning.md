---
title: Q-Bench
author: liliazhen669
date: 2025-05-01 16:00:00 +0800
categories: [Learning, RLearning]
tags: [paper] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

# 马尔可夫决策

## 马尔可夫过程

### 马尔可夫性质
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），用公式表示为 $P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1},...,S_{t})$ 。也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然时刻的状态$S_{t}$只与时刻$t$的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来

## 马尔可夫决策过程
在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值（value）**

### 策略
智能体（Agent）的策略（Policy）通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_t={a}|S_t=s)$ s是一个函数，表示在输入状态情况$s$下采取动作$a$的概率。当一个策略是确定性策略（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；当一个策略是随机性策略（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

### 状态价值函数
用 $V^{\pi}(s) $ 表示在 MDP 中基于策略 $\pi$ 的状态价值函数（state-value function），定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为：
$$
V^{\pi}(s) = \mathbb{E}[G_t|S_t=s]
$$

### 动作价值函数
不同于 MRP，在 MDP 中，由于动作的存在，需要额外定义一个**动作价值函数**（action-value function）。用 $Q^{\pi}(s, a)$ 表示在 MDP 中基于策略 $\pi$ 的对当前状态 $s$ 执行动作 $a$ 得到的期望回报：
$$
Q^{\pi}(s, a) = \mathbb{E}[G_t|S_t=s,A_t=a]
$$
状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果：
$$
V^{\pi}(s) = \sum_{a \in A}\pi(a|s) Q^{\pi}(s, a)
$$
使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$
Q^{\pi}(s, a) = r(s, a) + \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi}(s')
$$