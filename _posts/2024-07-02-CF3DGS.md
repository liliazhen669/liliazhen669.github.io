---
layout: post
title: pose free gs
date: 2024-07-02 15:51
category: 
author: 
tags: [gaussian_splatting]
summary: 
---

cf3dgs提到了几篇文章, 这几篇文章证明了可以同时估计相机参数并优化NerF, 但是要引入多种regularization terms和几何先验。大多数现有的方法优先从不同的相机位置来优化光线投射过程, 而非直接优化相机位姿。这是NerF中的隐式表示和光线跟踪的实现的性质导致的。这种间接地优化方法也自然地存在问题。

    Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4160–4169, 2023.

    Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. In ICCV, 2021.

    Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021.

作者提出了一个local 3dgs来估计相机的相对位姿。

作者给出了一个公式来揭示相机位姿与高斯点的3D刚体变换之间的关系。3D高斯的中心点为$\mu$, 相机位姿为$W$

$$
\mu_{2D}=K(W_{\mu})/W(W_{\mu})_z
$$

## 论文理解

其实结构还是简单的, 
流程是: 相邻的两帧如何估计相对位姿? 我们以序列的第0,1帧举例, 使用第0帧的RGB估计深度图$D_{0}$, 反投影点云并使用该点云初始化local gs, 得到$G_{0}$, 并在单张RGB图像$I_{0}$上迭代训练，得到收敛后的local gs, 表示为$G^{*}_{0}$。

然后使用下一帧的图像$I_{1}$来迭代, 但是此时, 将G^{*}_{0}的所有参数都冻结, 不再更新, 唯一更新的是两帧间的相对位姿。

## Code相关

~~~
        elif self.model_cfg.data_type == "custom":
            source_path = self.model_cfg.source_path
            cameras_intrinsic_file = os.path.join(source_path, "sparse/0", "cameras.bin")
            max_frames = 300
            # if os.path.exists(cameras_intrinsic_file):
            #     images = sorted(glob.glob(os.path.join(source_path, "images", "*.jpg")))
            #     if len(images)>max_frames:
            #         images = images[-max_frames:]
            #     cam_intrinsics = read_intrinsics_binary(cameras_intrinsic_file)
            #     intr = cam_intrinsics[1]
            #     focal_length_x = intr.params[0]
            #     focal_length_y = intr.params[1]
            #     height = intr.height
            #     width = intr.width
            #     intr_mat = np.array(
            #         [[focal_length_x, 0, width/2], [0, focal_length_y, height/2], [0, 0, 1]])
            #     self.intrinsic = intr_mat
            # else:
            images = sorted(glob.glob(os.path.join(source_path, "images/*.png")))  
            if len(images) > max_frames:
                interval = len(images) // max_frames
                images = images[::interval]
            print("Total images: ", len(images))
            width, height = Image.open(images[0]).size
~~~
在处理自定义数据时, 如果图像数量超过了最大图像数(默认为300), 则使用图像数量整除预设值, 并使用插值来采样图像。

**在测试自己数据集时这部分出现了错误, 比训练集的指定数量多了1帧, 参考自己仓库的修改后版本**

### 参数

默认的参数:
~~~
    densification_interval = 100
    densify_from_iter = 301
    densify_grad_threshold = 0.0002
    densify_interval = 500
    densify_until_iter = 130000
    depth_loss_type = 'invariant'
    feature_lr = 0.0025
    iterations = 300
    lambda_depth = 0.0
    lambda_dist_2nd_loss = 0.0
    lambda_dssim = 0.2
    lambda_pc = 0.0
    lambda_rgb_s = 0.0
    match_method = 'dense'
~~~

### relative pose

使用单帧收敛一个local gs, 然后使用下一帧的图像, 固定local gs的位置, 旋转学习率降低。 将local gs的一整个位姿self.P作为优化参数, 以李代数的形式传入: $LieGroupParam(x,y,z,qx,qy,qz,qw)$ 其中位移和旋转的初值是$[R|t]$

迭代300轮, 优化该相对位姿, 得到两帧之间的相对位姿$rel\_pose$, 然后乘以$gs_{global}$的上一帧位姿来得到当前位姿的结果。

Q: 两帧间的$rel\_pose$就这么得到了? 其效果如何？是否有保障? 效果与feature match比如何？

看一下迭代300轮收敛$rel\_pose$的过程: 
~~~
# 在300次迭代中更新LieGroup(self.P), 
for iteration in range(1, optim_opt.iterations+1):
    self.gs_render_local.gaussians.update_learning_rate(iteration)
    loss, rend_dict_ref, psnr_train = self.train_step(self.gs_render_local,
                                                              viewpoint_cam_ref, iteration,
                                                              pipe, optim_opt,
                                                              densify=False,
                                                              )
~~~

我们重点关注train_step的输入参数: 

~~~
self.gs_render_local:  上一帧RGB训练的local gs
viewpoint_cam_ref   :  当前帧的视角
iteration           :  当前的迭代轮次
pipe                :  gs相关, 不重要
optim_opt           :  优化相关, 不重要
densify=False       :  关闭densify, 意味着gs点不会增多
~~~

值得注意的是, 与训练local gs时使用的参数相比少了很多参数:
在训练前一帧的local gs时, 使用的参数有:
~~~
self.gs_render_local,
viewpoint_cam,                              前一帧的视角
iteration,
pipe,
optim_opt,
---以下是多出的部分---
depth_gt=self.mono_depth[view_idx_prev],    估计的单目深度图
update_gaussians=True,                      更新gs(但是默认就是True, 这里其实没有差别)
update_cam=False,                           不更新相机
update_distort=False,                       不更新畸变(默认就是false)
densify=False,                              不进行densify
~~~
因此两次迭代的区别在于: 
1. 第二次迭代使用了fix_position
2. 第一次迭代不更新相机, update_cam=False (只有eval_nvs时该选项才设置为了True)
3. 第一次迭代使用了mono_depth (但是权重？)

~~~
loss, rend_dict, psnr_train = self.train_step(self.gs_render_local,
                                                viewpoint_cam, iteration,
                                                pipe, optim_opt,
                                                depth_gt=self.mono_depth[view_idx_prev],
                                                update_gaussians=True,
                                                update_cam=False,
                                                updata_distort=False,
                                                densify=False, 
~~~

非常关键的一点是: **在初始的1000轮迭代中, 使用了估计得到的单目深度来对local gs进行了约束**

纠正: 只有init_two_view中的前一帧初始化时使用了深度图来监督, 而一般的两帧间add_view_v2中并没有, 

## COGS

### custom数据集测试

训练时间记录

![](/assets/img/2024-07-19-09-24-18.png)

![](/assets/img/2024-07-19-09-24-58.png)