---
title: MASt3r代码测试
date: 2024-04-10 20:51
category: 3D Gaussian Splatting
author: User
tags: [gaussian splatting]
summary: Summary of the article
math: true
image:
    path: /assets/img/mast3r-pre.png
---

[toc]


## 目标

尝试为mast3r添加mask支持, 仅mask中标记的前景区域参与匹配，生成对应关系。

## 代码部分

以下代码梳理和测试均以mast3r两帧图像match为例子

**https://github.com/naver/mast3r?tab=readme-ov-file#usage**

<details>
  <summary>2-ImageMatchDemo</summary>

```python
from mast3r.model import AsymmetricMASt3R
from mast3r.fast_nn import fast_reciprocal_NNs

import mast3r.utils.path_to_dust3r
from dust3r.inference import inference
from dust3r.utils.image import load_images

if __name__ == '__main__':
	device = 'cuda'
	schedule = 'cosine'
	lr = 0.01
	niter = 300

	model_name = "naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric"
	# you can put the path to a local checkpoint in model_name if needed
	model = AsymmetricMASt3R.from_pretrained(model_name).to(device)
	images = load_images(['dust3r/croco/assets/Chateau1.png', 'dust3r/croco/assets/Chateau2.png'], size=512)
	output = inference([tuple(images)], model, device, batch_size=1, verbose=False)

	# at this stage, you have the raw dust3r predictions
	view1, pred1 = output['view1'], output['pred1']
	view2, pred2 = output['view2'], output['pred2']

	desc1, desc2 = pred1['desc'].squeeze(0).detach(), pred2['desc'].squeeze(0).detach()

	# find 2D-2D matches between the two images
	matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,
												device=device, dist='dot', block_size=2**13)

	# ignore small border around the edge
	H0, W0 = view1['true_shape'][0]
	valid_matches_im0 = (matches_im0[:, 0] >= 3) & (matches_im0[:, 0] < int(W0) - 3) & (
		matches_im0[:, 1] >= 3) & (matches_im0[:, 1] < int(H0) - 3)

	H1, W1 = view2['true_shape'][0]
	valid_matches_im1 = (matches_im1[:, 0] >= 3) & (matches_im1[:, 0] < int(W1) - 3) & (
		matches_im1[:, 1] >= 3) & (matches_im1[:, 1] < int(H1) - 3)

	valid_matches = valid_matches_im0 & valid_matches_im1
	matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]

	# visualize a few matches
	import numpy as np
	import torch
	import torchvision.transforms.functional
	from matplotlib import pyplot as pl

	n_viz = 20
	num_matches = matches_im0.shape[0]
	match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)
	viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]

	image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)
	image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)

	viz_imgs = []
	for i, view in enumerate([view1, view2]):
		rgb_tensor = view['img'] * image_std + image_mean
		viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())

	H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]
	img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img = np.concatenate((img0, img1), axis=1)
	pl.figure()
	pl.imshow(img)
	cmap = pl.get_cmap('jet')
	for i in range(n_viz):
		(x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T
		pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)
	pl.show(block=True)
```

</details>

### dust3r forward

```python
class AsymmetricCroCo3DStereo (
    CroCoNet,
    huggingface_hub.PyTorchModelHubMixin,
    library_name="dust3r",
    repo_url="https://github.com/naver/dust3r",
    tags=["image-to-3d"],
):
    """ Two siamese encoders, followed by two decoders.
    The goal is to output 3d points directly, both images in view1's frame
    (hence the asymmetry).   
    """

	...

    def forward(self, view1, view2):
        # encode the two images --> B,S,D
        (shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)

        # combine all ref images into object-centric representation
        dec1, dec2 = self._decoder(feat1, pos1, feat2, pos2)

        with torch.cuda.amp.autocast(enabled=False):
            res1 = self._downstream_head(1, [tok.float() for tok in dec1], shape1)
            res2 = self._downstream_head(2, [tok.float() for tok in dec2], shape2)

        res2['pts3d_in_other_view'] = res2.pop('pts3d')  # predict view2's pts3d in view1's frame
        return res1, res2
```

view1, view2是dict类型, 包含4个key: **img, true_shape, idx, instance**

在输入到dust3r.forward之前已经经过了尺度缩放(默认缩放到最长边为512), 移动img到指定设备(gpu)上。

*_encode_symmetrized*获取两个视角的img图像(**B**atch_size, **C**hannel, **W**idth, **H**eight), 获取true_shape, torch.Size([1, 2])存储着图像的Width和Height.

```python
    def _encode_symmetrized(self, view1, view2):
        img1 = view1['img']
        img2 = view2['img']
        B = img1.shape[0]
        # Recover true_shape when available, otherwise assume that the img shape is the true one
		# 尝试获取true_shape, 如果没有则使用当前的图像大小作为true_shape
        shape1 = view1.get('true_shape', torch.tensor(img1.shape[-2:])[None].repeat(B, 1))
        shape2 = view2.get('true_shape', torch.tensor(img2.shape[-2:])[None].repeat(B, 1))
        # warning! maybe the images have different portrait/landscape orientations

        if is_symmetrized(view1, view2):
            # computing half of forward pass!'
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1[::2], img2[::2], shape1[::2], shape2[::2])
            feat1, feat2 = interleave(feat1, feat2)
            pos1, pos2 = interleave(pos1, pos2)
        else:
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1, img2, shape1, shape2)

        return (shape1, shape2), (feat1, feat2), (pos1, pos2)
```


## 编码图像对

### PatchEmbed

可以看到, 编码图像对时有两种: 

但是这两种不同只在于forward函数, 其都继承自croco的PatchEmbed类。

ViT的encoder部分其实很简单, 在model.py中匆匆一行
~~~
(shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)
~~~

对应到model.py中
~~~
def _encode_image(self, image, true_shape):
	# embed the image into patches  (x has size B x Npatches x C)
	x, pos = self.patch_embed(image, true_shape=true_shape)

	# add positional embedding without cls token
	assert self.enc_pos_embed is None

	# now apply the transformer encoder and normalization
	for blk in self.enc_blocks:
		x = blk(x, pos)

	x = self.enc_norm(x)
	return x, pos, None
~~~

首先patch_embed输入图像进行编码, 输出特征x和位置编码pos

以PatchEmbedDust3R为例子, 
~~~
class PatchEmbedDust3R(PatchEmbed):
    def forward(self, x, **kw):
        B, C, H, W = x.shape  # 确保图像的长宽能够被patch size整除
        assert H % self.patch_size[0] == 0, f"Input image height ({H}) is not a multiple of patch size ({self.patch_size[0]})."
        assert W % self.patch_size[1] == 0, f"Input image width ({W}) is not a multiple of patch size ({self.patch_size[1]})."
        x = self.proj(x)
        pos = self.position_getter(B, x.size(2), x.size(3), x.device)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        x = self.norm(x)
        return x, pos
~~~

可以看到, 输入的图像以patch的大小为步长提取特征, PositionGetter负责获得每个patch对应的位置。
~~~
self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
~~~

提取到特征之后, x的shape从(B, C, H, W)变为$(B, C_{embed}, H_p, W_p)$, 其中的$H_p$和$W_p$都是patch在纵横方向的数量。$H_p=(H-P)//P+1$, $W_p=(W-P)//P+1$

将后两维度输入到PositionGetter中, 可以得到patch所在的位置。

~~~
self.position_getter = PositionGetter()
pos = self.position_getter(B, x.size(2), x.size(3), x.device)
~~~

~~~
... 

# patch embedding
class PositionGetter(object):
    """ return positions of patches """

    def __init__(self):
        self.cache_positions = {}
        
    def __call__(self, b, h, w, device):
        if not (h,w) in self.cache_positions:
            x = torch.arange(w, device=device)
            y = torch.arange(h, device=device)
            self.cache_positions[h,w] = torch.cartesian_prod(y, x) # (h, w, 2)
        pos = self.cache_positions[h,w].view(1, h*w, 2).expand(b, -1, 2).clone()
        return pos
~~~

所以PatchEmbedDust3R其实就做了按Patch提取特征x和计算位置pos这么个事情。

### encoder blocks

但是提取特征后, encode阶段其实还没有完成。我们可以发现有x = blk(x, pos)这么一步

~~~
class AsymmetricCroCo3DStereo
	...
    def _encode_image(self, image, true_shape):
        # embed the image into patches  (x has size B x Npatches x C)
        x, pos = self.patch_embed(image, true_shape=true_shape)

        # add positional embedding without cls token
        assert self.enc_pos_embed is None

        # now apply the transformer encoder and normalization
        for blk in self.enc_blocks:
            x = blk(x, pos)

        x = self.enc_norm(x)
        return x, pos, None
~~~

这段代码来自croco, 也就是dust3r论文中提到的.

~~~
        self.enc_blocks = nn.ModuleList([
            Block(enc_embed_dim, enc_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, rope=self.rope)
            for i in range(enc_depth)])
~~~

rope是指(Relative Positional Encoding)相对位置编码。每一个Block就是attention+MLP, 默认的enc_depth=12, 也就是叠了12层Block.

所以结构其实很简单

~~~
input: img
	encoder(Patch embed) -> x, pos
	x, pos -> self-attn -> x
		norm(x) -> linear -> qkv
		linear(attn(qkv)) -> x
~~~

### 对称

在encoder中处处有对对称情况的处理, 一切的判断都来自函数
~~~
is_symmetrized(view1, view2)
~~~

那么什么情况才会被判定位两个view是对称的？
我们检查其位于misc.py的代码可以发现，对称其实检查的是view的instance项
**检查相邻的元素是否"成对,对称互换", 期待的对称情况是两个view的instance满足[a,b,c,d]和[b,a,d,c].** 即步长为2, 每两个, 都是互换的a,b和b,a、c,d和d,c

## Transformer Decoder



![](/assets/img/2024-10-10-15-22-16.png)

~~~
# transfer from encoder to decoder 
self.decoder_embed = nn.Linear(enc_embed_dim, dec_embed_dim, bias=True)
# transformer for the decoder 
self.dec_blocks = nn.ModuleList(
		[
			DecoderBlock(
					dec_embed_dim, 
					dec_num_heads, 
					mlp_ratio=mlp_ratio, 
					qkv_bias=True, 
					norm_layer=norm_layer, 
					norm_mem=norm_im2_in_dec, 
					rope=self.rope)
				for i in range(dec_depth)
		]
	)
~~~

decoder_embed做了一个简单的线性变换, 

~~~
    def _decoder(self, f1, pos1, f2, pos2):
        final_output = [(f1, f2)]  # before projection

        # project to decoder dim
        f1 = self.decoder_embed(f1)
        f2 = self.decoder_embed(f2)

        final_output.append((f1, f2))
        for blk1, blk2 in zip(self.dec_blocks, self.dec_blocks2):
            # img1 side
            f1, _ = blk1(*final_output[-1][::+1], pos1, pos2)
            # img2 side
            f2, _ = blk2(*final_output[-1][::-1], pos2, pos1)
            # store the result
            final_output.append((f1, f2))

        # normalize last output
        del final_output[1]  # duplicate with final_output[0]
        final_output[-1] = tuple(map(self.dec_norm, final_output[-1]))
        return zip(*final_output)
~~~


~~~
input: img
	x1, pos1 -> self-attn -> x1
	x1, pos1, x2, pos2 -> cross-attn -> x1
	x1 -> MLP
~~~

## GlobalAligner

### view1, view2, pred1, pred2之间的关系是什么？

model的输入输出为:

~~~
输入: 
    view1, view2
        img, (B, C, H, W) 
        true_shape, (B, 2) 每个Batch存储[H,W]
        idx, list, len==B, 存储两个图像的idx
        instance, list, len==B, 存储对应idx的字符类型

例子:
    images = load_images([img_path_1, img_path_2], size=512)  # 加载两张图像
    # 图像分别为images[0]['img'], images[1]['img']
    view1
        img: (2, 3, 512, 288) 
        true_shape: [[512, 288], [512, 288]]
        idx: [1, 0]
        instance: ['1', '0']
    view2
        img: (2, 3, 512, 288) 
        true_shape: [[512, 288], [512, 288]]
        idx: [0, 1]
        instance: ['0', '1']
        # 验证 torch.all(images[1]['img'][0] == view2['img'][1])

    # view1, view2中的idx对应着load_images中的图像的下标, 
    # 例如view1['idx']为[1, 0], 那么view1['img'][0]的idx就是1, 对应的图像就是images[1]['img']; 
    # 同理, view1['img'][1]对应的idx为0, 对应images[0]['img']
    # 可通过代码 torch.all(images[0]['img'][0] == view1['img'][1])
    # 进行验证

输出: 
    view1, view2
    pred1,
        pts3d, conf, desc, desc_conf
    pred2
        pts3d_in_other_view, conf, desc, desc_conf
    loss(使用时为None)
~~~


观察training.py中的loss, 

~~~
ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)

((loss1, msk1), (loss2, msk2)), details = self.pixel_loss(gt1, gt2, pred1, pred2, **kw)
~~~

先看Regr3D, 获取了gt1['camera_pose'], 并计算相机的逆, 将gt1和gt2的pts3d变换到相机1坐标系下, 以view1作为锚点。（也就是说, 原本gt1['pts3d']和gt2['pts3d']都是在世界坐标系下的, 通过相机1相机位姿的逆变换, 变为相机1坐标系下)

注意训练过程中的虽然输入的也是view1和view2, 但是此时的view其实是作为gt的, 其中包含了ground-truth信息, 比如世界坐标系下的点云pts3d, 相机位姿camera_pose, 场景的遮罩valid_mask.

get_pred_pts3d函数主要包含两部分内容, 第一部分, 获得当前相机的view下的点云。
如果pred1中有深度和相机内参, 则使用反投影直接得到pts3d; 
如果只有pred['pts3d'], 则直接使用;

---
由于输出的是pointmap, 所以我们可以把它当做一个深度图来直接进行可视化(仅可视化z)

![](/assets/img/2024-10-22-18-53-39.png)

添加conf_mask后的结果, 从左到右分别是对conf<1.0, conf<1.05, conf<1.10, conf<1.15, conf<1.2的过滤结果 

![](/assets/img/2024-10-23-09-23-58.png)

但是conf其实是相互的, 应当根据双向的conf来过滤, 哪些点来计算

---

如果use_pose为true, 使用pred.get('camera_pose')获得估计得到的当前的相机的位姿, 使用该位姿对得到的点云进行转换。下面的代码, pred1的点云直接使用pts3d, pred2的点云则是使用camera_pose对pts3d进行转换。

~~~
        pr_pts1 = get_pred_pts3d(gt1, pred1, use_pose=False)
        pr_pts2 = get_pred_pts3d(gt2, pred2, use_pose=True)
~~~

camera_pose == 相机坐标系到世界坐标系的变换$T_{wc}$, in_camera1则是世界坐标系到相机坐标系的变换。因此, gt_pts1和gt_pts2是在相机坐标系下的点云, gt1['pts3d']和gt2['pts3d']是世界坐标系下的点云。
~~~
        in_camera1 = inv(gt1['camera_pose'])
~~~
~~~
        gt_pts1 = geotrf(in_camera1, gt1['pts3d'])  # B,H,W,3
        gt_pts2 = geotrf(in_camera1, gt2['pts3d'])  # B,H,W,3
~~~

在返回的结果中, pr_pts1与gt_pts1计算loss, pr_pts2与gt_pts2计算loss, 说明pr_pts1和pr_pts2都是在相机1坐标系下的点云, 说明pred1['pts3d']是在相机1坐标系下的点云, pred2['pts3d']是在相机2坐标系下的点云, pred2['camera_pose']则是相机2->相机1的变换。


根据ConfLoss的计算loss的代码, 可以确认pred1['conf']对应的是img1, pred2['conf']对应的是img2

~~~
        # weight by confidence
        conf1, log_conf1 = self.get_conf_log(pred1['conf'][msk1])
        conf2, log_conf2 = self.get_conf_log(pred2['conf'][msk2])
        conf_loss1 = loss1 * conf1 - self.alpha * log_conf1  # conf_1*loss_1 - alpha*log(conf_1)
        conf_loss2 = loss2 * conf2 - self.alpha * log_conf2
~~~

**Q: 但是mast3r中的输出中的pts3d_in_other_view又是哪里来的？**

**A: 来自AsymmetricCroCo3DStereo, 也就是mast3r模型AsymmetricMASt3R的父类**

pts3d_in_other_view其实就是res2的pts3d, 注释也写明白了, 是预测的view2的点云, 但是在view1的坐标系下。

~~~
    def forward(self, view1, view2):
        # encode the two images --> B,S,D
        (shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)  # 对图像中的patch encode

        # combine all ref images into object-centric representation
        dec1, dec2 = self._decoder(feat1, pos1, feat2, pos2)

        with torch.cuda.amp.autocast(enabled=False):
            res1 = self._downstream_head(1, [tok.float() for tok in dec1], shape1)
            res2 = self._downstream_head(2, [tok.float() for tok in dec2], shape2)

        res2['pts3d_in_other_view'] = res2.pop('pts3d')  # predict view2's pts3d in view1's frame
        return res1, res2
~~~

所以我们可以得出**结论**:

~~~
输入: 
    view1, view2
        img, (B, C, H, W) 
        true_shape, (B, 2) 每个Batch存储[H,W]
        idx, list, len==B, 存储两个图像的idx
        instance, list, len==B, 存储对应idx的字符类型

输出: 
    view1, view2
    pred1,
        pts3d, view1[idx][0]在view1的点云, view1[idx][1]在view1的点云
        conf, 
        desc, 
        desc_conf
    pred2
        pts3d_in_other_view, view2[idx][0]在view1[idx][0]坐标系下的点云, view2[idx][1]在view1[idx][1]坐标系下的点云
        conf, 
        desc, 
        desc_conf
    loss(使用时为None)
~~~


![](/assets/img/2024-10-23-22-26-16.png)

<p style="text-align: center;">从左到右为 <strong>['pred1']['pts3d'][0]</strong>, <strong>['pred2']['pts3d_in_other_view'][0]</strong>, <strong>['pred1']['pts3d'][1]</strong>, <strong>['pred2']['pts3d_in_other_view'][1]</strong></p>

上面的结果我们按batch分别观察, [0]和[1]其实是区分不同的batch, 我们首先看batch0:

['pred1']['pts3d'][0]对应着view1中batch0的图像, view1[idx][0], 即图像1, 图像1的点云在图像1坐标系下。

['pred2']['pts3d_in_other_view'][0]则是view2[idx][0], 即图像0的点云在图像1坐标系下。

到这里其实隐约能够感觉到, 为什么要使用make_pair将图像对进行交换, 是为了确保图像即便交换顺序也不会因为encoder-decoder而得到不同的结果。

![](/assets/img/2024-10-24-15-30-29.png)

~~~
例子:
    images = load_images([img_path_1, img_path_2], size=512)  # 加载两张图像
    # 图像分别为images[0]['img'], images[1]['img']
    view1
        img: (2, 3, 512, 288) 
        true_shape: [[512, 288], [512, 288]]
        idx: [1, 0]
        instance: ['1', '0']
    view2
        img: (2, 3, 512, 288) 
        true_shape: [[512, 288], [512, 288]]
        idx: [0, 1]
        instance: ['0', '1']
        # 验证 torch.all(images[1]['img'][0] == view2['img'][1])
~~~

输出的图像, 输出的pointmap(pts3d和pts3d_in_other_view)的对应以及所在坐标系如下图所示:

![](/assets/img/2024-10-28-10-21-30.png)

### 映射关系

最近在修改mask mast3r, 使其能够支持超过>2张的rgb+mask输入:

映射关系如下:

~~~
输入:
    3张图像路径

images
    [
        images[0]
            img
                Tensor, shape: torch.Size([1, 3, 512, 288])
            true_shape
                ndarray, shape: (1, 2)
            idx
                int
            instance
                str
        images[1]
        images[2]
    ]
~~~
$C^2_3$能够得到6对图像, pairs的长度为6
~~~
pairs
    [
        pairs[0]
            pairs[0][0]
                img
                    Tensor, shape: torch.Size([1, 3, 512, 288])
                true_shape
                    ndarray, shape: (1, 2)
                idx
                    int
                instance
                    str
            pairs[0][1]
        pairs[1]
        pairs[2]
        ...
        pairs[5]
    ]
~~~


~~~
output
    view1
        img
            Tensor, shape: torch.Size([6, 3, 512, 288])
        true_shape
            Tensor, shape: torch.Size([6, 2])
        idx
            list, length: 6
        instance
            list, length: 6
    view2
        img
            Tensor, shape: torch.Size([6, 3, 512, 288])
        true_shape
            Tensor, shape: torch.Size([6, 2])
        idx
            list, length: 6
        instance
            list, length: 6
    pred1
        pts3d
            Tensor, shape: torch.Size([6, 512, 288, 3])
        conf
            Tensor, shape: torch.Size([6, 512, 288])
        desc
            Tensor, shape: torch.Size([6, 512, 288, 24])
        desc_conf
            Tensor, shape: torch.Size([6, 512, 288])
    pred2
        conf
            Tensor, shape: torch.Size([6, 512, 288])
        desc
            Tensor, shape: torch.Size([6, 512, 288, 24])
        desc_conf
            Tensor, shape: torch.Size([6, 512, 288])
        pts3d_in_other_view
            Tensor, shape: torch.Size([6, 512, 288, 3])
    loss
        NoneType
~~~

我之前认为使用view1对应pred1, pred1使用view1中对应的idx去寻找mask和image即可。但似乎有错误。

~~~
view1
    img
        Tensor, shape: torch.Size([2, 3, 512, 288])
    true_shape
        Tensor, shape: torch.Size([2, 2])
    idx
        list, length: 2
    instance
        list, length: 2
view2
    img
        Tensor, shape: torch.Size([2, 3, 512, 288])
    true_shape
        Tensor, shape: torch.Size([2, 2])
    idx
        list, length: 2
    instance
        list, length: 2
pred1
    pts3d
        Tensor, shape: torch.Size([2, 512, 288, 3])
    conf
        Tensor, shape: torch.Size([2, 512, 288])
    desc
        Tensor, shape: torch.Size([2, 512, 288, 24])
    desc_conf
        Tensor, shape: torch.Size([2, 512, 288])
pred2
    conf
        Tensor, shape: torch.Size([2, 512, 288])
    desc
        Tensor, shape: torch.Size([2, 512, 288, 24])
    desc_conf
        Tensor, shape: torch.Size([2, 512, 288])
    pts3d_in_other_view
        Tensor, shape: torch.Size([2, 512, 288, 3])
loss
    NoneType
~~~
其中
~~~
output['view1']['idx']
[1, 0]
output['view2']['idx']
[0, 1]
~~~

## mask问题

观察inference输出的output中的pts3d和pts3d_in_other_view后发现, 作者之前在[issue](https://github.com/naver/dust3r/issues/13)中回答说单纯地使用mask重置conf可能不会有效的原因了。

![](/assets/img/2024-10-24-20-30-46.png)

或者说它可以发挥作用，仅当mask分割出的物体是一个静态的物体，和背景的相对位置是不变的。此时背景参与回归和match其实并不会影响对物体的匹配和重建。但刚才的这个说法也并不严谨，**经过测试, 如果对输入的图像进行裁剪, 提取特征后估计的相机内参也会受到影响, 因此在提取特征前尽量不要对图像进行裁剪**

pts3d和conf是inference之后网络输出的结果, 估计相对位姿和重建使用同一坐标框架下的两个view的点云, 但是此时的背景信息其实已经发挥过作用了(Vit encoder提取patch信息)。在某种程度上, 匹配已经发生过了, 因此使用mask对conf进行置0操作(在指数中是置1)不会完全起作用，因为此时回归得到的pts3d已经包含了背景的信息, 即：pts3d的回归和之后的对齐操作其实已经被mask去除的信息影响过了

![](/assets/img/2024-10-24-21-06-32.png)

<p style="text-align: center;">color+crop+whiteBG: 在inference之前, 将mask去除部分的像素统一设置为白色</p>

顺带一提, 上图中, 反光问题其实也会严重影响匹配

![](/assets/img/pts_crop_whitebg.gif)

## 前背景分离与patch embedding

我在测试过程中，尝试将背景的颜色设置为(0,0,0), 并通过将其conf设置为1.0的方式使其不参与匹配。

我们首先看看, 背景颜色设置为0, 对其估计得到的pointmap有什么影响。

**输入白色背景后的匹配结果**

然后，我们发现，在边缘处, 有一些点的回归坐标向前飞出, 非常接近相机。有意思的是，这些“飞点”的情况只发生在pts3d中, 在pts3d_in_other中并不存在。可能是测试的过少？

**飞点图像**

## 焦距估计



## desc_conf和conf

desc_conf, conf, pts3d 都来自于postprocess函数. 这一段来自于CroCo3DStereo的_downstream_head, 即decoder已经结束, 根据下游任务生成对应的head的阶段。

~~~
def postprocess(out, depth_mode, conf_mode, desc_dim=None, desc_mode='norm', two_confs=False, desc_conf_mode=None):
    if desc_conf_mode is None:
        desc_conf_mode = conf_mode
    fmap = out.permute(0, 2, 3, 1)  # B,H,W,D
    res = dict(pts3d=reg_dense_depth(fmap[..., 0:3], mode=depth_mode))
    if conf_mode is not None:
        res['conf'] = reg_dense_conf(fmap[..., 3], mode=conf_mode)
    if desc_dim is not None:
        start = 3 + int(conf_mode is not None)
        res['desc'] = reg_desc(fmap[..., start:start + desc_dim], mode=desc_mode)
        if two_confs:
            res['desc_conf'] = reg_dense_conf(fmap[..., start + desc_dim], mode=desc_conf_mode)
        else:
            res['desc_conf'] = res['conf'].clone()
    return res
~~~

**Q:desc_conf和conf有什么不同？**

conf和desc_conf都来自函数reg_dense_conf, 但是其输入有所不同:

~~~
    res['conf'] = reg_dense_conf(fmap[..., 3], mode=conf_mode)
~~~

~~~
    if two_confs:
        res['desc_conf'] = reg_dense_conf(fmap[..., start + desc_dim], mode=desc_conf_mode)
    else:
        res['desc_conf'] = res['conf'].clone()
~~~

我们运行一个测试的例子看一下计算的过程，首先看参数的输入：

~~~
out: [1, 29, 512, 288]
depth_mode: ('exp', -inf, inf)
conf_mode: ('exp', 1, inf)
desc_dim: 24
desc_mode: 'norm'
two_confs: True
desc_conf_mode: ('exp', 0, inf)
~~~

输入的out是dpt输出的pts3d，与local_features的拼接. fmap就是feature map, 是对out变换为(B, H, W, D)得到的。



~~~
self.head_local_features = Mlp(in_features=idim,
                                hidden_features=int(hidden_dim_factor * idim),
                                out_features=(self.local_feat_dim + self.two_confs) * self.patch_size**2)
local_features = self.head_local_features(cat_output)  # B,S,D
local_features = local_features.transpose(-1, -2).view(B, -1, H // self.patch_size, W // self.patch_size)
local_features = F.pixel_shuffle(local_features, self.patch_size)  # B,d,H,W

out = torch.cat([pts3d, local_features], dim=1)
~~~


## Sparse GA

在mast3r的demo中, 有这么一段注释:

~~~
# Sparse GA (forward mast3r -> matching -> 3D optim -> 2D refinement -> triangulation)
~~~

可以发现整个sparse global alignment分为4个阶段:

1.


## dust3r

### 优化

论文中的公式5

$$
\chi^* = \arg\min_{\chi, P, \sigma} \sum_{e \in \mathcal{E}} \sum_{v \in e} \sum_{i=1}^{HW} C_i^{v,e} \left\| \chi_i^v - \sigma_e P_e X_i^{v,e} \right\|.
$$

https://github.com/naver/dust3r/blob/main/dust3r/cloud_opt/base_opt.py

~~~
class BasePCOptimizer (nn.Module):
	...
    def forward(self, ret_details=False):
        pw_poses = self.get_pw_poses()  # cam-to-world
        pw_adapt = self.get_adaptors()
        proj_pts3d = self.get_pts3d()
        # pre-compute pixel weights
        weight_i = {i_j: self.conf_trf(c) for i_j, c in self.conf_i.items()}
        weight_j = {i_j: self.conf_trf(c) for i_j, c in self.conf_j.items()}

        loss = 0
        if ret_details:
            details = -torch.ones((self.n_imgs, self.n_imgs))

        for e, (i, j) in enumerate(self.edges):
            i_j = edge_str(i, j)
            # distance in image i and j
            aligned_pred_i = geotrf(pw_poses[e], pw_adapt[e] * self.pred_i[i_j])
            aligned_pred_j = geotrf(pw_poses[e], pw_adapt[e] * self.pred_j[i_j])
            li = self.dist(proj_pts3d[i], aligned_pred_i, weight=weight_i[i_j]).mean()
            lj = self.dist(proj_pts3d[j], aligned_pred_j, weight=weight_j[i_j]).mean()
            loss = loss + li + lj

            if ret_details:
                details[i, j] = li + lj
        loss /= self.n_edges  # average over all pairs

        if ret_details:
            return loss, details
        return loss
~~~

其中get_pw_poses获得的就是公式中的$\sigma_e P_e$, 搭配adapt用来将pointmap变换为全局对齐的pointmap并计算loss.

在BasePCOptimizer的初始化函数中，我们可以发现
~~~
self.POSE_DIM = 7
self.pw_poses = nn.Parameter(rand_pose((self.n_edges, 1+self.POSE_DIM)))  # pairwise poses
~~~

其在POSE_DIM(4维度的四元数旋转+3维的位移)的基础上再+1，这一个额外的维度在后面的函数get_pw_scale中使用，它被用来优化尺度.

~~~
    def get_pw_scale(self):
        scale = self.pw_poses[:, -1].exp()  # (n_edges,)
        scale = scale * self.get_pw_norm_scale_factor()
        return scale
~~~



~~~
    def _get_poses(self, poses):
        # normalize rotation
        Q = poses[:, :4]
        T = signed_expm1(poses[:, 4:7])
        RT = roma.RigidUnitQuat(Q, T).normalize().to_homogeneous()
        return RT

    def get_pw_poses(self):  # cam to world
        RT = self._get_poses(self.pw_poses)
        scaled_RT = RT.clone()
        scaled_RT[:, :3] *= self.get_pw_scale().view(-1, 1, 1)  # scale the rotation AND translation
        return scaled_RT
~~~

其中signed_expm1对输入的(n, 3)的n个位移，先记录符号sign(负数返回-1, 0返回0, 正数返回1), 然后torch.expm1对绝对值计算$e^{|x|}-1$

~~~
def signed_expm1(x):
    sign = torch.sign(x)
    return sign * torch.expm1(torch.abs(x))
~~~

综上, 完成了操作

$$
sign(x)(e^{|x|}-1)
$$

主要是为了数值稳定性

然后使用四元数Q和位移T创建一个刚体变换矩阵RT(齐次形式, 4×4), 返回后与缩放因子相乘self.get_pw_scale().view(-1, 1, 1)

可以发现，forward函数主要是根据已经得到的pose计算loss, 优化的代码在别处

### 最小生成树

在官方的demo中使用的init方法是mst, 使用的是最小生成树, 迭代300轮。

~~~
    def compute_global_alignment(self, init=None, niter_PnP=10, **kw):
        if init is None:
            pass
        elif init == 'msp' or init == 'mst':
            init_fun.init_minimum_spanning_tree(self, niter_PnP=niter_PnP)
        elif init == 'known_poses':
            init_fun.init_from_known_poses(self, min_conf_thr=self.min_conf_thr,
                                           niter_PnP=niter_PnP)
        else:
            raise ValueError(f'bad value for {init=}')

        return global_alignment_loop(self, **kw)
~~~

minimum_spanning_tree

来到init_im_pose.py, 这里我们能够发现minimum_spanning_tree


### 估计焦距f

在dust3r中, 输入一个(W,)

~~~
im_focals[i] = estimate_focal(pred_i[i_j])
~~~

有两种模式: median和最小二乘法估计焦距

$$
f_x = \frac{u \cdot z}{x}, \quad f_y = \frac{v \cdot z}{y}
$$

## mast3r

使用fast_reciprocal_NNs函数的地方主要有两个

1.visloc.py中的coarse_match和fine_match

2.sparse_ga.py中的extract_correspondences提取对应关系时

### visloc

#### 数据集准备

https://github.com/naver/mast3r/issues/20

### coarse-to-fine

