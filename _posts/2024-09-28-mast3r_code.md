---
title: MASt3r代码测试
date: 2024-04-10 20:51
category: 3D Gaussian Splatting
author: User
tags: [gaussian splatting]
summary: Summary of the article
math: true
---

[toc]


## 目标

尝试为mast3r添加mask支持, 仅mask中标记的前景区域参与匹配，生成对应关系。

## 代码部分

以下代码梳理和测试均以mast3r两帧图像match为例子

**https://github.com/naver/mast3r?tab=readme-ov-file#usage**

<details>
  <summary>2-ImageMatchDemo</summary>

```python
from mast3r.model import AsymmetricMASt3R
from mast3r.fast_nn import fast_reciprocal_NNs

import mast3r.utils.path_to_dust3r
from dust3r.inference import inference
from dust3r.utils.image import load_images

if __name__ == '__main__':
	device = 'cuda'
	schedule = 'cosine'
	lr = 0.01
	niter = 300

	model_name = "naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric"
	# you can put the path to a local checkpoint in model_name if needed
	model = AsymmetricMASt3R.from_pretrained(model_name).to(device)
	images = load_images(['dust3r/croco/assets/Chateau1.png', 'dust3r/croco/assets/Chateau2.png'], size=512)
	output = inference([tuple(images)], model, device, batch_size=1, verbose=False)

	# at this stage, you have the raw dust3r predictions
	view1, pred1 = output['view1'], output['pred1']
	view2, pred2 = output['view2'], output['pred2']

	desc1, desc2 = pred1['desc'].squeeze(0).detach(), pred2['desc'].squeeze(0).detach()

	# find 2D-2D matches between the two images
	matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,
												device=device, dist='dot', block_size=2**13)

	# ignore small border around the edge
	H0, W0 = view1['true_shape'][0]
	valid_matches_im0 = (matches_im0[:, 0] >= 3) & (matches_im0[:, 0] < int(W0) - 3) & (
		matches_im0[:, 1] >= 3) & (matches_im0[:, 1] < int(H0) - 3)

	H1, W1 = view2['true_shape'][0]
	valid_matches_im1 = (matches_im1[:, 0] >= 3) & (matches_im1[:, 0] < int(W1) - 3) & (
		matches_im1[:, 1] >= 3) & (matches_im1[:, 1] < int(H1) - 3)

	valid_matches = valid_matches_im0 & valid_matches_im1
	matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]

	# visualize a few matches
	import numpy as np
	import torch
	import torchvision.transforms.functional
	from matplotlib import pyplot as pl

	n_viz = 20
	num_matches = matches_im0.shape[0]
	match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)
	viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]

	image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)
	image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)

	viz_imgs = []
	for i, view in enumerate([view1, view2]):
		rgb_tensor = view['img'] * image_std + image_mean
		viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())

	H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]
	img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img = np.concatenate((img0, img1), axis=1)
	pl.figure()
	pl.imshow(img)
	cmap = pl.get_cmap('jet')
	for i in range(n_viz):
		(x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T
		pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)
	pl.show(block=True)
```

</details>

### dust3r forward

```python
class AsymmetricCroCo3DStereo (
    CroCoNet,
    huggingface_hub.PyTorchModelHubMixin,
    library_name="dust3r",
    repo_url="https://github.com/naver/dust3r",
    tags=["image-to-3d"],
):
    """ Two siamese encoders, followed by two decoders.
    The goal is to output 3d points directly, both images in view1's frame
    (hence the asymmetry).   
    """

	...

    def forward(self, view1, view2):
        # encode the two images --> B,S,D
        (shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)

        # combine all ref images into object-centric representation
        dec1, dec2 = self._decoder(feat1, pos1, feat2, pos2)

        with torch.cuda.amp.autocast(enabled=False):
            res1 = self._downstream_head(1, [tok.float() for tok in dec1], shape1)
            res2 = self._downstream_head(2, [tok.float() for tok in dec2], shape2)

        res2['pts3d_in_other_view'] = res2.pop('pts3d')  # predict view2's pts3d in view1's frame
        return res1, res2
```

view1, view2是dict类型, 包含4个key: **img, true_shape, idx, instance**

在输入到dust3r.forward之前已经经过了尺度缩放(默认缩放到最长边为512), 移动img到指定设备(gpu)上。

*_encode_symmetrized*获取两个视角的img图像(**B**atch_size, **C**hannel, **W**idth, **H**eight), 获取true_shape, torch.Size([1, 2])存储着图像的Width和Height.

```python
    def _encode_symmetrized(self, view1, view2):
        img1 = view1['img']
        img2 = view2['img']
        B = img1.shape[0]
        # Recover true_shape when available, otherwise assume that the img shape is the true one
		# 尝试获取true_shape, 如果没有则使用当前的图像大小作为true_shape
        shape1 = view1.get('true_shape', torch.tensor(img1.shape[-2:])[None].repeat(B, 1))
        shape2 = view2.get('true_shape', torch.tensor(img2.shape[-2:])[None].repeat(B, 1))
        # warning! maybe the images have different portrait/landscape orientations

        if is_symmetrized(view1, view2):
            # computing half of forward pass!'
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1[::2], img2[::2], shape1[::2], shape2[::2])
            feat1, feat2 = interleave(feat1, feat2)
            pos1, pos2 = interleave(pos1, pos2)
        else:
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1, img2, shape1, shape2)

        return (shape1, shape2), (feat1, feat2), (pos1, pos2)
```


## 编码图像对

### PatchEmbed

可以看到, 编码图像对时有两种: 

但是这两种不同只在于forward函数, 其都继承自croco的PatchEmbed类。

ViT的encoder部分其实很简单, 在model.py中匆匆一行
~~~
(shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)
~~~

对应到model.py中
~~~
def _encode_image(self, image, true_shape):
	# embed the image into patches  (x has size B x Npatches x C)
	x, pos = self.patch_embed(image, true_shape=true_shape)

	# add positional embedding without cls token
	assert self.enc_pos_embed is None

	# now apply the transformer encoder and normalization
	for blk in self.enc_blocks:
		x = blk(x, pos)

	x = self.enc_norm(x)
	return x, pos, None
~~~

首先patch_embed输入图像进行编码, 输出特征x和位置编码pos

以PatchEmbedDust3R为例子, 
~~~
class PatchEmbedDust3R(PatchEmbed):
    def forward(self, x, **kw):
        B, C, H, W = x.shape  # 确保图像的长宽能够被patch size整除
        assert H % self.patch_size[0] == 0, f"Input image height ({H}) is not a multiple of patch size ({self.patch_size[0]})."
        assert W % self.patch_size[1] == 0, f"Input image width ({W}) is not a multiple of patch size ({self.patch_size[1]})."
        x = self.proj(x)
        pos = self.position_getter(B, x.size(2), x.size(3), x.device)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC
        x = self.norm(x)
        return x, pos
~~~

可以看到, 输入的图像以patch的大小为步长提取特征, PositionGetter负责获得每个patch对应的位置。
~~~
self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
~~~

提取到特征之后, x的shape从(B, C, H, W)变为$(B, C_{embed}, H_p, W_p)$, 其中的$H_p$和$W_p$都是patch在纵横方向的数量。$H_p=(H-P)//P+1$, $W_p=(W-P)//P+1$

将后两维度输入到PositionGetter中, 可以得到patch所在的位置。

~~~
self.position_getter = PositionGetter()
pos = self.position_getter(B, x.size(2), x.size(3), x.device)
~~~

~~~
... 

# patch embedding
class PositionGetter(object):
    """ return positions of patches """

    def __init__(self):
        self.cache_positions = {}
        
    def __call__(self, b, h, w, device):
        if not (h,w) in self.cache_positions:
            x = torch.arange(w, device=device)
            y = torch.arange(h, device=device)
            self.cache_positions[h,w] = torch.cartesian_prod(y, x) # (h, w, 2)
        pos = self.cache_positions[h,w].view(1, h*w, 2).expand(b, -1, 2).clone()
        return pos
~~~

所以PatchEmbedDust3R其实就做了按Patch提取特征x和计算位置pos这么个事情。

### encoder blocks

但是提取特征后, encode阶段其实还没有完成。我们可以发现有x = blk(x, pos)这么一步

~~~
class AsymmetricCroCo3DStereo
	...
    def _encode_image(self, image, true_shape):
        # embed the image into patches  (x has size B x Npatches x C)
        x, pos = self.patch_embed(image, true_shape=true_shape)

        # add positional embedding without cls token
        assert self.enc_pos_embed is None

        # now apply the transformer encoder and normalization
        for blk in self.enc_blocks:
            x = blk(x, pos)

        x = self.enc_norm(x)
        return x, pos, None
~~~

这段代码来自croco, 也就是dust3r论文中提到的.

~~~
        self.enc_blocks = nn.ModuleList([
            Block(enc_embed_dim, enc_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, rope=self.rope)
            for i in range(enc_depth)])
~~~

rope是指(Relative Positional Encoding)相对位置编码。每一个Block就是attention+MLP, 默认的enc_depth=12, 也就是叠了12层Block.

所以结构其实很简单

~~~
input: img
	encoder(Patch embed) -> x, pos
	x, pos -> self-attn -> x
		norm(x) -> linear -> qkv
		linear(attn(qkv)) -> x
~~~

### 对称

在encoder中处处有对对称情况的处理, 一切的判断都来自函数
~~~
is_symmetrized(view1, view2)
~~~

那么什么情况才会被判定位两个view是对称的？
我们检查其位于misc.py的代码可以发现，对称其实检查的是view的instance项
**检查相邻的元素是否"成对,对称互换", 期待的对称情况是两个view的instance满足[a,b,c,d]和[b,a,d,c].** 即步长为2, 每两个, 都是互换的a,b和b,a、c,d和d,c

## Transformer Decoder



![](/assets/img/2024-10-10-15-22-16.png)

~~~
# transfer from encoder to decoder 
self.decoder_embed = nn.Linear(enc_embed_dim, dec_embed_dim, bias=True)
# transformer for the decoder 
self.dec_blocks = nn.ModuleList(
		[
			DecoderBlock(
					dec_embed_dim, 
					dec_num_heads, 
					mlp_ratio=mlp_ratio, 
					qkv_bias=True, 
					norm_layer=norm_layer, 
					norm_mem=norm_im2_in_dec, 
					rope=self.rope)
				for i in range(dec_depth)
		]
	)
~~~

decoder_embed做了一个简单的线性变换, 

~~~
    def _decoder(self, f1, pos1, f2, pos2):
        final_output = [(f1, f2)]  # before projection

        # project to decoder dim
        f1 = self.decoder_embed(f1)
        f2 = self.decoder_embed(f2)

        final_output.append((f1, f2))
        for blk1, blk2 in zip(self.dec_blocks, self.dec_blocks2):
            # img1 side
            f1, _ = blk1(*final_output[-1][::+1], pos1, pos2)
            # img2 side
            f2, _ = blk2(*final_output[-1][::-1], pos2, pos1)
            # store the result
            final_output.append((f1, f2))

        # normalize last output
        del final_output[1]  # duplicate with final_output[0]
        final_output[-1] = tuple(map(self.dec_norm, final_output[-1]))
        return zip(*final_output)
~~~


~~~
input: img
	x1, pos1 -> self-attn -> x1
	x1, pos1, x2, pos2 -> cross-attn -> x1
	x1 -> MLP
~~~

## Sparse GA

在mast3r的demo中, 有这么一段注释:

~~~
# Sparse GA (forward mast3r -> matching -> 3D optim -> 2D refinement -> triangulation)
~~~

可以发现整个sparse global alignment分为4个阶段:

1.


## dust3r

### 优化

论文中的公式5

$$
\chi^* = \arg\min_{\chi, P, \sigma} \sum_{e \in \mathcal{E}} \sum_{v \in e} \sum_{i=1}^{HW} C_i^{v,e} \left\| \chi_i^v - \sigma_e P_e X_i^{v,e} \right\|.
$$

https://github.com/naver/dust3r/blob/main/dust3r/cloud_opt/base_opt.py

~~~
class BasePCOptimizer (nn.Module):
	...
    def forward(self, ret_details=False):
        pw_poses = self.get_pw_poses()  # cam-to-world
        pw_adapt = self.get_adaptors()
        proj_pts3d = self.get_pts3d()
        # pre-compute pixel weights
        weight_i = {i_j: self.conf_trf(c) for i_j, c in self.conf_i.items()}
        weight_j = {i_j: self.conf_trf(c) for i_j, c in self.conf_j.items()}

        loss = 0
        if ret_details:
            details = -torch.ones((self.n_imgs, self.n_imgs))

        for e, (i, j) in enumerate(self.edges):
            i_j = edge_str(i, j)
            # distance in image i and j
            aligned_pred_i = geotrf(pw_poses[e], pw_adapt[e] * self.pred_i[i_j])
            aligned_pred_j = geotrf(pw_poses[e], pw_adapt[e] * self.pred_j[i_j])
            li = self.dist(proj_pts3d[i], aligned_pred_i, weight=weight_i[i_j]).mean()
            lj = self.dist(proj_pts3d[j], aligned_pred_j, weight=weight_j[i_j]).mean()
            loss = loss + li + lj

            if ret_details:
                details[i, j] = li + lj
        loss /= self.n_edges  # average over all pairs

        if ret_details:
            return loss, details
        return loss
~~~

其中get_pw_poses获得的就是公式中的$\sigma_e P_e$, 搭配adapt用来将pointmap变换为全局对齐的pointmap并计算loss.

在BasePCOptimizer的初始化函数中，我们可以发现
~~~
self.POSE_DIM = 7
self.pw_poses = nn.Parameter(rand_pose((self.n_edges, 1+self.POSE_DIM)))  # pairwise poses
~~~

其在POSE_DIM(4维度的四元数旋转+3维的位移)的基础上再+1，这一个额外的维度在后面的函数get_pw_scale中使用，它被用来优化尺度.

~~~
    def get_pw_scale(self):
        scale = self.pw_poses[:, -1].exp()  # (n_edges,)
        scale = scale * self.get_pw_norm_scale_factor()
        return scale
~~~



~~~
    def _get_poses(self, poses):
        # normalize rotation
        Q = poses[:, :4]
        T = signed_expm1(poses[:, 4:7])
        RT = roma.RigidUnitQuat(Q, T).normalize().to_homogeneous()
        return RT

    def get_pw_poses(self):  # cam to world
        RT = self._get_poses(self.pw_poses)
        scaled_RT = RT.clone()
        scaled_RT[:, :3] *= self.get_pw_scale().view(-1, 1, 1)  # scale the rotation AND translation
        return scaled_RT
~~~

其中signed_expm1对输入的(n, 3)的n个位移，先记录符号sign(负数返回-1, 0返回0, 正数返回1), 然后torch.expm1对绝对值计算$e^{|x|}-1$

~~~
def signed_expm1(x):
    sign = torch.sign(x)
    return sign * torch.expm1(torch.abs(x))
~~~

综上, 完成了操作

$$
sign(x)(e^{|x|}-1)
$$

主要是为了数值稳定性

然后使用四元数Q和位移T创建一个刚体变换矩阵RT(齐次形式, 4×4), 返回后与缩放因子相乘self.get_pw_scale().view(-1, 1, 1)

可以发现，forward函数主要是根据已经得到的pose计算loss, 优化的代码在别处

### 最小生成树

在官方的demo中使用的init方法是mst, 使用的是最小生成树, 迭代300轮。

~~~
    def compute_global_alignment(self, init=None, niter_PnP=10, **kw):
        if init is None:
            pass
        elif init == 'msp' or init == 'mst':
            init_fun.init_minimum_spanning_tree(self, niter_PnP=niter_PnP)
        elif init == 'known_poses':
            init_fun.init_from_known_poses(self, min_conf_thr=self.min_conf_thr,
                                           niter_PnP=niter_PnP)
        else:
            raise ValueError(f'bad value for {init=}')

        return global_alignment_loop(self, **kw)
~~~

minimum_spanning_tree

来到init_im_pose.py, 这里我们能够发现minimum_spanning_tree


### 估计焦距f

在dust3r中, 输入一个(W,)

~~~
im_focals[i] = estimate_focal(pred_i[i_j])
~~~

有两种模式: median和最小二乘法估计焦距

$$
f_x = \frac{u \cdot z}{x}, \quad f_y = \frac{v \cdot z}{y}
$$

## mast3r

使用fast_reciprocal_NNs函数的地方主要有两个

1.visloc.py中的coarse_match和fine_match

2.sparse_ga.py中的extract_correspondences提取对应关系时

### visloc

#### 数据集准备

https://github.com/naver/mast3r/issues/20

### coarse-to-fine

