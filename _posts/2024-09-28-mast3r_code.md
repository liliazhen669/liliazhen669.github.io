---
title: MASt3r代码测试
date: 2024-04-10 20:51
category: 3D Gaussian Splatting
author: User
tags: [gaussian splatting]
summary: Summary of the article
math: true
---

[toc]


## 目标

尝试为mast3r添加mask支持, 仅mask中标记的前景区域参与匹配，生成对应关系。

## 代码部分

以下代码梳理和测试均以mast3r两帧图像match为例子

**https://github.com/naver/mast3r?tab=readme-ov-file#usage**

<details>
  <summary>2-ImageMatchDemo</summary>

```python
from mast3r.model import AsymmetricMASt3R
from mast3r.fast_nn import fast_reciprocal_NNs

import mast3r.utils.path_to_dust3r
from dust3r.inference import inference
from dust3r.utils.image import load_images

if __name__ == '__main__':
	device = 'cuda'
	schedule = 'cosine'
	lr = 0.01
	niter = 300

	model_name = "naver/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric"
	# you can put the path to a local checkpoint in model_name if needed
	model = AsymmetricMASt3R.from_pretrained(model_name).to(device)
	images = load_images(['dust3r/croco/assets/Chateau1.png', 'dust3r/croco/assets/Chateau2.png'], size=512)
	output = inference([tuple(images)], model, device, batch_size=1, verbose=False)

	# at this stage, you have the raw dust3r predictions
	view1, pred1 = output['view1'], output['pred1']
	view2, pred2 = output['view2'], output['pred2']

	desc1, desc2 = pred1['desc'].squeeze(0).detach(), pred2['desc'].squeeze(0).detach()

	# find 2D-2D matches between the two images
	matches_im0, matches_im1 = fast_reciprocal_NNs(desc1, desc2, subsample_or_initxy1=8,
												device=device, dist='dot', block_size=2**13)

	# ignore small border around the edge
	H0, W0 = view1['true_shape'][0]
	valid_matches_im0 = (matches_im0[:, 0] >= 3) & (matches_im0[:, 0] < int(W0) - 3) & (
		matches_im0[:, 1] >= 3) & (matches_im0[:, 1] < int(H0) - 3)

	H1, W1 = view2['true_shape'][0]
	valid_matches_im1 = (matches_im1[:, 0] >= 3) & (matches_im1[:, 0] < int(W1) - 3) & (
		matches_im1[:, 1] >= 3) & (matches_im1[:, 1] < int(H1) - 3)

	valid_matches = valid_matches_im0 & valid_matches_im1
	matches_im0, matches_im1 = matches_im0[valid_matches], matches_im1[valid_matches]

	# visualize a few matches
	import numpy as np
	import torch
	import torchvision.transforms.functional
	from matplotlib import pyplot as pl

	n_viz = 20
	num_matches = matches_im0.shape[0]
	match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)
	viz_matches_im0, viz_matches_im1 = matches_im0[match_idx_to_viz], matches_im1[match_idx_to_viz]

	image_mean = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)
	image_std = torch.as_tensor([0.5, 0.5, 0.5], device='cpu').reshape(1, 3, 1, 1)

	viz_imgs = []
	for i, view in enumerate([view1, view2]):
		rgb_tensor = view['img'] * image_std + image_mean
		viz_imgs.append(rgb_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())

	H0, W0, H1, W1 = *viz_imgs[0].shape[:2], *viz_imgs[1].shape[:2]
	img0 = np.pad(viz_imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img1 = np.pad(viz_imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant', constant_values=0)
	img = np.concatenate((img0, img1), axis=1)
	pl.figure()
	pl.imshow(img)
	cmap = pl.get_cmap('jet')
	for i in range(n_viz):
		(x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T
		pl.plot([x0, x1 + W0], [y0, y1], '-+', color=cmap(i / (n_viz - 1)), scalex=False, scaley=False)
	pl.show(block=True)
```

</details>

### dust3r forward

```python
class AsymmetricCroCo3DStereo (
    CroCoNet,
    huggingface_hub.PyTorchModelHubMixin,
    library_name="dust3r",
    repo_url="https://github.com/naver/dust3r",
    tags=["image-to-3d"],
):
    """ Two siamese encoders, followed by two decoders.
    The goal is to output 3d points directly, both images in view1's frame
    (hence the asymmetry).   
    """

	...

    def forward(self, view1, view2):
        # encode the two images --> B,S,D
        (shape1, shape2), (feat1, feat2), (pos1, pos2) = self._encode_symmetrized(view1, view2)

        # combine all ref images into object-centric representation
        dec1, dec2 = self._decoder(feat1, pos1, feat2, pos2)

        with torch.cuda.amp.autocast(enabled=False):
            res1 = self._downstream_head(1, [tok.float() for tok in dec1], shape1)
            res2 = self._downstream_head(2, [tok.float() for tok in dec2], shape2)

        res2['pts3d_in_other_view'] = res2.pop('pts3d')  # predict view2's pts3d in view1's frame
        return res1, res2
```

view1, view2是dict类型, 包含4个key: **img, true_shape, idx, instance**

在输入到dust3r.forward之前已经经过了尺度缩放(默认缩放到最长边为512), 移动img到指定设备(gpu)上。

*_encode_symmetrized*获取两个视角的img图像(**B**atch_size, **C**hannel, **W**idth, **H**eight), 获取true_shape, torch.Size([1, 2])存储着图像的Width和Height.

```python
    def _encode_symmetrized(self, view1, view2):
        img1 = view1['img']
        img2 = view2['img']
        B = img1.shape[0]
        # Recover true_shape when available, otherwise assume that the img shape is the true one
		# 尝试获取true_shape, 如果没有则使用当前的图像大小作为true_shape
        shape1 = view1.get('true_shape', torch.tensor(img1.shape[-2:])[None].repeat(B, 1))
        shape2 = view2.get('true_shape', torch.tensor(img2.shape[-2:])[None].repeat(B, 1))
        # warning! maybe the images have different portrait/landscape orientations

        if is_symmetrized(view1, view2):
            # computing half of forward pass!'
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1[::2], img2[::2], shape1[::2], shape2[::2])
            feat1, feat2 = interleave(feat1, feat2)
            pos1, pos2 = interleave(pos1, pos2)
        else:
            feat1, feat2, pos1, pos2 = self._encode_image_pairs(img1, img2, shape1, shape2)

        return (shape1, shape2), (feat1, feat2), (pos1, pos2)
```


## 编码图像对

### PatchEmbed

可以看到, 编码图像对时有两种: 

但是这两种不同只在于forward函数, 其都继承自croco的PatchEmbed类。


## Sparse GA

在mast3r的demo中, 有这么一段注释:

~~~
# Sparse GA (forward mast3r -> matching -> 3D optim -> 2D refinement -> triangulation)
~~~

可以发现整个sparse global alignment分为4个阶段:

1.