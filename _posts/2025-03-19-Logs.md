---
title: Logs when coding
author: liliazhen669
date: 2025-03-20 16:00:00 +0800
categories: [Problems, Error]
tags: [logs] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

如何从from_pretrained中加载safetensors：先下载hugging face的model card到本地，然后将名字修改为默认的比如“diffusion_model.safetensors”，[safetensors参考](https://github.com/OpenDocCN/huggingface-doc-zh/blob/master/docs/dfsr_0.26/dfsr026_019.md)，[safetensors参考2](https://huggingface.co/docs/diffusers/main/using-diffusers/using_safetensors)

运行Lora时，train_text_to_image_lora.py raise ValueError("Attempting to unscale FP16 gradients.") ，解决报错问题：不从命令行传递mixed_presision参数，直接改用默认参数：[参考链接](https://github.com/huggingface/diffusers/issues/6363)

`PIL.Image`格式的图片在调用resize方法的时候,高宽参数的输入顺序是**w，h**而不是**h,w**

`repr()` 方法可以将列表对象 `list` 转换为其字符串表示形式，从而可以将整个列表变为一整行存到txt文件中，使用示例如下：
```python
filename = 'output.txt'
lists = ['645_8', '345_5', '345_0', '1345_1', '1648_2']
with open(filename, 'w', encoding='utf-8') as file:
    file.write(repr(lists))
```

## Logs
分离Image的光照是一件困难的任务（Image Intrinsic Decomposition），如何提取一个合适的照明向量来作为条件以生成和背景能够和谐融合的肖像是一困难的任务

Latent Intrinsics Emerge from Training to Relight （简记为LIE） 训练了一个UNet网络来从输入图片中提取Lihting code，其输入是相同场景在不同光照条件下的配对图片，Encoder的中间变量分别是代表Albedo的多尺度Intrinsic Features和代表Lighting code的Extrinsic Vector，

而 LumiNet 同样使用的是配对的训练数据，直接使用了LIE的预训练权重来提取Reference Image的Lighting code，然后设计了一个ControlNet将提取的Lightig Code作为条件传入到Latent Diffusion Model的去噪网络中

受此启发，可以训练一个类似的网络来提取Lighting Code，不同的是该网络需要适合Portrait的特征（LIE的权重是在室内图片数据集上训练得到的，且分辨率为256）；同样地，室内数据集中的Lighting Code会来自明显的光源，而Portrait的光源不一定在Image中是可见的，因此模型想要从Reference Image中提取光源会更难，仅仅使用LIE的权重来进行微调是不够的（此处可以进行消融实验来进行验证）

