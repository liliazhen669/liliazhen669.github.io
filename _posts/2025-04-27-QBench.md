---
title: Q-Bench
author: liliazhen669
date: 2025-04-27 16:00:00 +0800
categories: [Learning, IQA]
tags: [paper] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

# Introduction and Method

多模态大语言模型（Multi-modality Large Language Models，后续简称多模态大模型）能够提供强大的通用级别视觉感知/理解能力，甚至可以通过自然语言与人类进行无缝对话和互动。虽然多模态大模型的这些能力已经在多个视觉语言任务中得到了探索和验证，例如图像字幕、视觉问题回答、跨模态关联，以及传统的视觉任务，如图像分类或分割，但大多数关注点都集中在对视觉内容的高级感知和理解上。与此同时，多模态大模型在 low-level 视觉感知和理解方面的能力仍然不清楚，这在图像质量评估（IQA）以及感知视觉失真（噪音、模糊）等相关任务上发挥着重要作用，以及其他 low-level 属性（颜色、光照、构图、风格等），这些属性可能与自然照片的美学和情感以及人们对新兴计算机图形生成或 AI 生成图像的偏好有关。

这些 low-level 视觉能力与广泛的应用密切相关，例如推荐、摄像系统指导，或视觉质量增强。因此，评估目前这些通用基础模型在 low-level 视觉感知和理解方面的能力至关重要，理想情况下，可以减轻大量人力资源为每个具体的 low-level 任务提供反馈。

本文提出的能够测量low-level 视觉感知以及理解MLLMs能力的benchmark围绕一个**核心问题**展开：*How do MLLMs emulate human ability related to low-level visual perception and understanding*？

![img-1](https://developer.qcloudimg.com/http-save/yehe-1324186/3570cb24673bfbf352cde7ec401f1f53.png "图1")


简单来说，答案是语言，这是多模态大模型的基本属性。具体而言，作者定义多模态大模型在low-level视觉方面的几种新兴语言能力如下：

- **Perception** of Low-level Attributes. 如图 1(a)所示，多模态大模型应该能够像人类一样准确地回答与 low-level 属性相关的简单问题，例如在查询“这张图像清晰吗？”时回答“不清晰”。为实现这一个目的，构建了包含10个不同来源踪迹2990张图片的LLVisionQA dataset，LLVisionQA 包括三种问题类型：*Yes-or-NO Question*, *What question* 和 *How questions*, 以及通过两个axes来将low-level concerns分成四个quadrants：（1）distortions（比如blur，noise）以及其他low-level属性（比如color，lighting， composition等）；（2）global perception（比如整张图片的sharpness）以及local content-related in-context perception（比如是否红花是对焦的）

- **Description** via Natural Language. 如图1(b)所示，多模态大模型应该能够像人类一样用自然语言描述图像的质量和其他low-level信息。这些描述应该既完整又准确(complete and accurate)。
- Precise **Assessment** Aligned with Human Opinions. 如图 1(c)所示，多模态大模型应该能够为图像预测可量化的质量分数，这些分数可以与人类对 low-level 视觉外观的均值意见分数（Mean Opinion Score）一致。为实现这一目标，作者利用了大量现有的IQA databases。特别地，作者话注意到MLLMs难以提供有效的可量化的输出，是否被命令直接用texts评分还是提供numerical outputs，为解决这一问题，作者提出在两个最频繁tokens（good and poor）的logis上提取softmax pooling 结果来作为质量预测 。实验证明，本文提出的方法的评价结果相比与MLLMs的直接token输出（通过argmax）更贴近人类结果，即连接了传统的IQA任务以及新兴的大语言模型

本文的**主要贡献**总结如下：
- 建立了一个关于多模态大模型 low-level 感知能力的基准。为了实现这一目标，构建了首个平衡且综合的 LLVisionQA 数据集，其中包含 2,990 张图像，每张图像都配有一个与 low-level相关的问题和正确以及错误的候选答案。

- 定义了一个LLDSEscribe数据集，用于评估多模态大模型的 low-level 描述能力，其中包括一个包含 499 张图像的 LLDescription 数据集，其中包含由专家标注的长篇的黄金质量描述，以及通过 GPT 辅助评估多模态大模型描述的完整性、准确性和相关性，与黄金描述进行比较。
- 提出了一种统一的基于 softmax 的质量预测策略以评估精确的质量评估能力，该策略适用于所有多模态大模型，基于多模态大模型的概率输出。通过我们的实验验证了该策略的有效性，该策略为通用多模态大模型与传统 IQA 任务之间建立了桥梁，使它们可以输出可量化的图片质量分数。

# CONSTRUCTING THE Q-BENCH

## BENCHMARK ON LOW-LEVEL PERCEPTION ABILITY

在Q-Bench的第一个任务中，作者评估MLLMs的low-level感知能力以检查是否能够回答low-level相关的简单且自然的问询。为实现这个目的，作者首先从10个不同来源的数据集中挑选了共2990张图片，然后对于每一张图片（I），收集一个low-level-ralted问题（Q），一个正确答案（C），以及1-3个错误答案（F）。元组（I，Q，C，F）便构成了**LLVisionQA**的训练数据集，特别地，LLVisionQA中的问题包含了四种distinct low-level  concerns以及三种问题类型。由训练数据产生的输出最后会被送人到GPT中来检查正确性，以下是这一任务的详细内容

### QUADRANTS FOR LOW-LEVEL VISUAL CONCERNS

**Axis 1: Distortions vs Other Low-level Attributes.**主要区分两种low-level感知属性：1）technical distortions，比如直接能够直接影像图像质量的low-level characteristics；2）（aesthetic-related）美学相关的其他Low-level attributes

**Axis 2: Global Perception vs Local In-context Perception.**在最近关于low-level vision的研究中发现，人类对低级视觉的感知往往与高级语境理解交织在一起，比如一个clear sky可能会缺失复杂texture但缺表现出exceptional clarity。除此之外外，局部low-level外观也可能与整体外观有所不同。考虑到这些差异，作者整理了一些**local in-context perception**问题（图 2 右），这些问题需要 MLLMs 掌握内容或其他背景才能正确回答，而其他问题则归类为**global perception**问题（图 2 左）

### GPT-ASSISTED EVALUATION PROCESS
测试MLLMs的评估能力时，输入的格式为：
*User:How is the clarity of the image? (Question) [IMAGE TOKEN] (Image)  Choose between one of the following options: A. High (Correct) B. Medium(Wrong) C. Low(Wrong)*

## BENCHMARK ON LOW-LEVEL DESCRIPTION ABILITY

在Q-Bench的第二个任务中，作者评估MLLMs的语言描述能力，该任务是image captioning（用自然语言描述图像内容）的子任务，但是特别关注于low-level 描述数据集。为此，作者特别制作了一个LLDescribe数据集，该数据集总计499张图片，包括了由评分专家攥写的，平均长度为58词的*gloden*描述。有了这一个数据集，作者便能够使用一个单模态GPT，在三种维度：completeness, preciseness, 和 relevance 下测量MLLMs输出的质量

### EVALUATION WITH SINGLE-MODAL GPT
通过使用LLDescribe数据集，作者将多模态问题转换为仅文本环境，具体做法是将多模态大模型（MLLM）的输出与单模态GPT的*golden*描述在三个维度上进行匹配：(1) Completeness。鼓励包含更多与*golden*描述匹配的信息。(2) Preciseness。对与黄金描述存在争议的信息进行惩罚。(3) Relevance。更多的多模态大模型输出部分应与低级信息相关，而不是其他内容。每个维度的得分范围为[0,1,2]。作者对每次单一评估进行5轮重复，并收集加权平均值作为最终得分。



## BENCHMARK ON PRECISE QUALITY ASSESSMENT ABILITY

### WEAK MEASURABILITY OF MLLM OUTPUTS

在Q-Bench中，作者期望公平地比较不同MLLMs在各种low-level appeara上的评估能力。因此，作者的原则是定义一个统一的、最简单的指令，使其适用于所有的MLLMs和所有的图像质量评估（IQA）数据集。在这一原则下，作者在Shikra和LLaVA-v1上针对LLVisionQA进行了一些小型的实验，采用了两种简单的指令策略：(A) 直接指令，其中提示被设计为简单的 *“Rate the quality of the image”*。最高频的答案是 **Good**（78%）和 **Poor**（20%），其他输出几乎可以忽略不计。(B) 数值指令，我们特别指示进行数值评分，提示为：*"Score the quality of the image from 1 to 5, with 1 as lowest and 5 as highest."*。在数值策略下，最高频的答案是5（84%）、1（9%）和3（5%）；尽管在评分范围内，分数2和4的频率均少于1%。这些实验**表明MLLM输出的可测量性较弱**，因为答案在统计上表现出1）偏向正面，2）偏向极端，以及3）仅有两个有效评分尺度。因此，有必要探索扩展策略，使MLLMs能为低级别评估提供真正可量化的输出。

###

鉴于上述观察，作者设计了softmax-based evaluation strategy 以减少偏见和缺乏评分尺度带来的负面影响。首先，在直接指令策略内设计策略，因为这比数值指令更通用且偏见更少。该策略基于观察到两个最高频率的输出，即 Good 和 Poor，可以被认为是较好和较差人类感知的锚点，并且直接策略可以在 [SCORE TOKEN] 位置近似为一个二元分类问题，或者技术上在此位置上进行“好” (x 
good SCORE TOKEN
​
 ) 和“差” (x 
poor SCORE TOKEN
​
 ) 的对数值之间的极大值选择。在我们修订的策略中，我们将极大值选择修改为软最大化，以收集更好的可量化评分：

# Conclusion

作者在本项研究中构建了 Q-Bench，这是一个用于检验多模态大模型在 low-level 视觉能力方面的进展的基准。作者期望这些大型基础模型可以成为通用智能，最终能够减轻人类的努力，因此作者提出多模态大模型应该具备三种重要且不同的能力：对 low-level 视觉属性的准确感知、对 low-level 视觉信息的精确和完整的语言描述，以及对图像质量的定量评估。为了评估这些能力，作者收集了两个多模态的 low-level 视觉基准数据集，并提出了一个基于 Softmax 的统一的多模态大模型定量 IQA 策略。作者的评估证明，即使没有任何针对 low-level 的具体训练，一些杰出的多模态大模型仍然具有不错的 low-level 能力。然而，这些多模态大模型要成为真正可靠的通用 low-level 视觉助手还有很长的路要走。作者衷心希望 Q-Bench 中发现的观察结果可以激发未来的大模型增强 low-level 感知和理解能力。