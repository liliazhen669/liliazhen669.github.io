---
title: Q-Bench
author: liliazhen669
date: 2025-04-27 16:00:00 +0800
categories: [Learning, IQA]
tags: [paper] # TAG names should always be lowercas
render_with_liquid: false
math: true
---

# Introduction and Method

多模态大语言模型（Multi-modality Large Language Models，后续简称多模态大模型）能够提供强大的通用级别视觉感知/理解能力，甚至可以通过自然语言与人类进行无缝对话和互动。虽然多模态大模型的这些能力已经在多个视觉语言任务中得到了探索和验证，例如图像字幕、视觉问题回答、跨模态关联，以及传统的视觉任务，如图像分类或分割，但大多数关注点都集中在对视觉内容的高级感知和理解上。与此同时，多模态大模型在 low-level 视觉感知和理解方面的能力仍然不清楚，这在图像质量评估（IQA）以及感知视觉失真（噪音、模糊）等相关任务上发挥着重要作用，以及其他 low-level 属性（颜色、光照、构图、风格等），这些属性可能与自然照片的美学和情感以及人们对新兴计算机图形生成或 AI 生成图像的偏好有关。

这些 low-level 视觉能力与广泛的应用密切相关，例如推荐、摄像系统指导，或视觉质量增强。因此，评估目前这些通用基础模型在 low-level 视觉感知和理解方面的能力至关重要，理想情况下，可以减轻大量人力资源为每个具体的 low-level 任务提供反馈。

本文提出的能够测量low-level 视觉感知以及理解MLLMs能力的benchmark围绕一个**核心问题**展开：*How do MLLMs emulate human ability related to low-level visual perception and understanding*？

![img-1](https://developer.qcloudimg.com/http-save/yehe-1324186/3570cb24673bfbf352cde7ec401f1f53.png "图1")


简单来说，答案是语言，这是多模态大模型的基本属性。具体而言，作者定义多模态大模型在low-level视觉方面的几种新兴语言能力如下：

- **Perception** of Low-level Attributes. 如图 1(a)所示，多模态大模型应该能够像人类一样准确地回答与 low-level 属性相关的简单问题，例如在查询“这张图像清晰吗？”时回答“不清晰”。为实现这一个目的，构建了包含10个不同来源踪迹2990张图片的LLVisionQA dataset，LLVisionQA 包括三种问题类型：*Yes-or-NO Question*, *What question* 和 *How questions*, 以及通过两个axes来将low-level concerns分成四个quadrants：（1）distortions（比如blur，noise）以及其他low-level属性（比如color，lighting， composition等）；（2）global perception（比如整张图片的sharpness）以及local content-related in-context perception（比如是否红花是对焦的）

- **Description** via Natural Language. 如图1(b)所示，多模态大模型应该能够像人类一样用自然语言描述图像的质量和其他low-level信息。这些描述应该既完整又准确(complete and accurate)。
- Precise **Assessment** Aligned with Human Opinions. 如图 1(c)所示，多模态大模型应该能够为图像预测可量化的质量分数，这些分数可以与人类对 low-level 视觉外观的均值意见分数（Mean Opinion Score）一致。为实现这一目标，作者利用了大量现有的IQA databases。特别地，作者话注意到MLLMs难以提供有效的可量化的输出，是否被命令直接用texts评分还是提供numerical outputs，为解决这一问题，作者提出在两个最频繁tokens（good and poor）的logis上提取softmax pooling 结果来作为质量预测 。实验证明，本文提出的方法的评价结果相比与MLLMs的直接token输出（通过argmax）更贴近人类结果，即连接了传统的IQA任务以及新兴的大语言模型

本文的**主要贡献**总结如下：
- 建立了一个关于多模态大模型 low-level 感知能力的基准。为了实现这一目标，构建了首个平衡且综合的 LLVisionQA 数据集，其中包含 2,990 张图像，每张图像都配有一个与 low-level相关的问题和正确以及错误的候选答案。

- 定义了一个LLDSEscribe数据集，用于评估多模态大模型的 low-level 描述能力，其中包括一个包含 499 张图像的 LLDescription 数据集，其中包含由专家标注的长篇的黄金质量描述，以及通过 GPT 辅助评估多模态大模型描述的完整性、准确性和相关性，与黄金描述进行比较。
- 提出了一种统一的基于 softmax 的质量预测策略以评估精确的质量评估能力，该策略适用于所有多模态大模型，基于多模态大模型的概率输出。通过我们的实验验证了该策略的有效性，该策略为通用多模态大模型与传统 IQA 任务之间建立了桥梁，使它们可以输出可量化的图片质量分数。

# CONSTRUCTING THE Q-BENCH

## BENCHMARK ON LOW-LEVEL PERCEPTION ABILITY

## BENCHMARK ON LOW-LEVEL DESCRIPTION ABILITY

## BENCHMARK ON PRECISE QUALITY ASSESSMENT ABILITY

# Conclusion

作者在本项研究中构建了 Q-Bench，这是一个用于检验多模态大模型在 low-level 视觉能力方面的进展的基准。作者期望这些大型基础模型可以成为通用智能，最终能够减轻人类的努力，因此作者提出多模态大模型应该具备三种重要且不同的能力：对 low-level 视觉属性的准确感知、对 low-level 视觉信息的精确和完整的语言描述，以及对图像质量的定量评估。为了评估这些能力，作者收集了两个多模态的 low-level 视觉基准数据集，并提出了一个基于 Softmax 的统一的多模态大模型定量 IQA 策略。作者的评估证明，即使没有任何针对 low-level 的具体训练，一些杰出的多模态大模型仍然具有不错的 low-level 能力。然而，这些多模态大模型要成为真正可靠的通用 low-level 视觉助手还有很长的路要走。作者衷心希望 Q-Bench 中发现的观察结果可以激发未来的大模型增强 low-level 感知和理解能力。